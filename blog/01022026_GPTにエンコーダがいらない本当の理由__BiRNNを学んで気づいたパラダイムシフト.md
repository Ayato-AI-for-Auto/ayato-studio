---
title: "GPTにエンコーダがいらない本当の理由 —— BiRNNを学んで気づいたパラダイムシフト"
date: "01/02/2026 21:00:00"
---

# GPTにエンコーダがいらない本当の理由 —— BiRNNを学んで気づいたパラダイムシフト

Category: 考察・コラム LLM | Tags: GPT Decoder-only

これまでの記事で、「BiRNN（双方向RNN）」と「Attention（注意機構）」について学んできました。  
これらは、翻訳モデル（Encoder-Decoder）において、入力を一度「エンコーダ」で理解してから、それを「デコーダ」で翻訳するという流れの中で進化してきた技術です。

しかし、ふと疑問に思いました。  
**「今の最強AIであるChatGPT（GPTモデル）には、エンコーダがない（Decoder-only）らしい。なぜ入力専用の部品がないのに、あれほど文脈を理解できるのか？」**

BiRNNの仕組み（未来と過去の情報を結合する）を理解した上で、GPTの構造について考えを巡らせていたとき、ある一つの仮説が降りてきました。

GPTは「入力（プロンプト）」を、入力データとしてではなく、  
「デコーダがたった今、自分で生成し終えた過去の文章（生成物）」とみなすことで、  
エンコーダなしでの生成を可能にしているのではないか？

調べてみると、この直感はまさに**GPTの本質（Decoder-only Architecture）**そのものでした。

## 1. 従来の常識：EncoderとDecoderの分業

これまでの翻訳モデルなどは、役割が明確に分かれていました。

* **Encoder:** 「読む人」。入力文（$x$）を読んで、意味のかたまり（ベクトル）に変換する。
* **Decoder:** 「書く人」。ベクトルを受け取って、翻訳文（$y$）を書く。

BiRNNの記事で書いたように、Encoderは「未来の情報（文末）」までカンニングして、完璧な意味ベクトルを作ることが仕事でした。

## 2. GPTの革命：すべてを「続きを書く」ことに統一

一方、GPTは**Decoderしか持っていません**。では、どうやって「質問（プロンプト）」を理解しているのでしょうか？

答えは、**「質問文も、すでに誰かが書いた『物語の前半部分』として扱う」**ことでした。

図解：Encoder-Decoder vs Decoder-only (GPT)

       Encoder-Decoder  Encoder   Decoder 入力: "こんにちは" 出力: "Hello"   GPT (Decoder-only)  Decoder 入力: "こんにちは" (過去)   出力: "Hello" (未来)  KV Cache

### 3. Prefill：実質的なエンコード処理

GPTがプロンプトを受け取った瞬間、内部では**「Prefill（プレフィル）」**と呼ばれる処理が走ります。  
これは、「ここまでの文章（プロンプト）はもう確定した過去の出来事だよ」として、一気にモデルに流し込み、その文脈情報（KeyとValue）をメモリ（Attention Cache）に保存する作業です。

つまり、**「プロンプトをキャッシュに焼き付ける作業」こそが、実質的なエンコーダの役割**を果たしているのです。

## 4. 「続きを書く」ことの凄さ

この仕組みの画期的な点は、**「入力」と「出力」の境界線が消滅した**ことです。

* **翻訳タスク：**「英語：Hello、日本語：[　]」の続きを書く。
* **要約タスク：**「長い文章... 要約すると：[　]」の続きを書く。
* **小説執筆：**「昔々あるところに[　]」の続きを書く。

このパラダイムシフトにより、GPTは世界中のあらゆるテキスト（小説、ブログ、コード）を「教師データ」として使えるようになりました。特定のラベル（正解データ）が不要になったのです。

モデル内にある大量の学習データで構築された「アノテーション情報（重み）」が、プロンプトという「短期記憶」を高度に解釈し、キャッシュとして保持する。それを使って続きを生成する。

これが、エンコーダを持たないGPTが、あれほど賢く振る舞える理由でした。

結論

基礎（BiRNN）を学んだからこそ、「未来を見ない」という制約の中で  
GPTがどうやって文脈を扱っているかが見えました。  
**「過去（プロンプト）を徹底的に理解すれば、未来（生成）は予測できる」**  
それがGPTの哲学なのかもしれません。