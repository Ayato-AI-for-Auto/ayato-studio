<!DOCTYPE html>
<html lang="ja">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>固定長ベクトルの呪縛を解く —— 論文「Neural Machine Translation by Jointly Learning to Align and Translate」解説 | Ayato AI Studio</title>
    <link rel="stylesheet" href="../index.css">
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&family=Noto+Sans+JP:wght@400;700&display=swap"
        rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', 'Noto Sans JP', sans-serif;
            background: #0a0a0a;
            color: #f0f0f0;
            margin: 0;
            padding: 0;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
        }

        header {
            padding: 2rem 0;
            border-bottom: 1px solid #333;
            margin-bottom: 2rem;
        }

        .back-link {
            color: #00ffaa;
            text-decoration: none;
            font-size: 0.9rem;
        }

        h1 {
            font-size: 2.5rem;
            margin: 0.5rem 0;
            background: linear-gradient(to right, #ffffff, #888888);
            background-clip: text;
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .meta {
            color: #888;
            font-size: 0.9rem;
        }

        .content {
            line-height: 1.8;
            font-size: 1.1rem;
        }

        .content h2 {
            margin-top: 2rem;
            color: #00ffaa;
        }

        .content h3 {
            margin-top: 1.5rem;
            color: #0066ff;
        }

        .content a {
            color: #00ffaa;
        }

        .content code {
            background: #222;
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: monospace;
        }

        .content pre {
            background: #222;
            padding: 1rem;
            border-radius: 8px;
            overflow-x: auto;
        }

        .content img {
            max-width: 100%;
            border-radius: 8px;
            margin: 1rem 0;
        }

        .footer {
            margin-top: 4rem;
            padding-top: 2rem;
            border-top: 1px solid #333;
            text-align: center;
            color: #666;
        }
    </style>
</head>

<body>
    <div class="container">
        <header>
            <a href="../index.html" class="back-link">← Back to Portal</a>
            <h1>固定長ベクトルの呪縛を解く —— 論文「Neural Machine Translation by Jointly Learning to Align and Translate」解説</h1>
            <div class="meta">Published on "12/31/2025 21:00:00" by Ayato</div>
        </header>
        <div class="content">
            <h1>固定長ベクトルの呪縛を解く —— 論文「Neural Machine Translation by Jointly Learning to Align and Translate」解説</h1>
<p>Category: 論文解説 Attention | Tags: Bahdanau Attention Transformer 機械翻訳</p>
<p>前回の記事では、双方向RNN（BiRNN）を使って「文脈を完全に理解した単語ベクトル」を作る仕組みについて解説しました。</p>
<p>今回は、そのベクトルを使って、現代のAI（Transformer/GPT）の基礎となった革命的技術<strong>「Attention Mechanism（注意機構）」</strong>がどのように生まれたのか、その原点となる論文 <em>Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et al., 2015)</em> を解説します。</p>
<h2>1. 従来の課題：固定長ベクトルのボトルネック</h2>
<p>この論文が出る前（2014年頃まで）のニューラル翻訳モデル（Encoder-Decoder）には、致命的な欠点がありました。</p>
<p>それは、<strong>「どんなに長い小説でも、たった一つの固定長ベクトル（例：1000個の数字）に圧縮しなければならない」</strong>という制約です。</p>
<ul>
<li>短い文：「I have a pen.」 → ベクトルAに圧縮（余裕）</li>
<li>長い文：「(50単語以上の長い契約書など)」 → ベクトルBに圧縮（<strong>情報が溢れて忘れてしまう！</strong>）</li>
</ul>
<p>エンコーダが必死に圧縮した情報を、デコーダは無理やり解凍して翻訳していましたが、文が長くなると最初の方の内容を忘れてしまい、翻訳精度がガタ落ちしていたのです。</p>
<h2>2. 解決策：圧縮をやめて「リスト」で渡す</h2>
<p>この論文の著者は考えました。<strong>「無理に一つに圧縮するからダメなんだ。全部の単語の情報をそのままリストで渡して、翻訳するときに必要な場所をカンニング（検索）させればいいじゃないか」</strong>と。</p>
<p>パラダイムシフト</p>
<p><strong>Before:</strong> エンコーダは文全体を一つのベクトルにする。<br />
<strong>After:</strong> エンコーダは「単語ごとのベクトル（アノテーション）のリスト」を作る。</p>
<p>ここで登場するのが、前回解説したBiRNNで作った「アノテーション (h_j)」のリストです。</p>
<h2>3. 仕組み：Attentionの数式解剖</h2>
<p>デコーダは、翻訳する各ステップで、このリストの中から「今どこを見るべきか」を自動的に計算します。これを数式で追ってみましょう。</p>
<h3>Step 1: 相性診断（エネルギー (e_{ij})）</h3>
<p>デコーダの現在の状態（(s_{i-1})）と、原文の各単語（(h_j)）の相性を計算します。</p>
<p>[ e_{ij} = a(s_{i-1}, h_j) ]</p>
<p>これは「今、翻訳したい単語にとって、原文の (j) 番目の単語はどれくらい重要？」というスコアを出しています。</p>
<h3>Step 2: 確率への変換（重み (\alpha_{ij})）</h3>
<p>スコアをSoftmax関数に通して、合計が100%になる確率（重み）に変換します。</p>
<p>[ \alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})} ]</p>
<p>例えば、「apple」を訳すときは原文の「りんご」への重みが0.9、「私は」への重みが0.05...といった具合になります。</p>
<h3>Step 3: 情報のブレンド（コンテキストベクトル (c_i)）</h3>
<p>最後に、重み (\alpha) に従って、原文の情報を混ぜ合わせます。</p>
<p>[ c_i = \sum_{j=1}^{T_x} \alpha_{ij} h_j ]</p>
<p>この (c_i) こそが、<strong>「今の翻訳に必要な情報だけを抽出したベクトル」</strong>です。デコーダはこれを使って次の単語を生成します。</p>
<h2>4. アライメントの可視化（ヒートマップ）</h2>
<p>この仕組みの素晴らしい点は、モデルがどこを見ているかが人間にも分かることです。以下は、論文中の実験結果を再現したヒートマップです。</p>
<p><strong>原文(英):</strong> "European Economic Area" (欧州経済領域)<br />
<strong>翻訳(仏):</strong> "zone économique européenne"</p>
<p>図：Attention重み（(\alpha_{ij})）の可視化</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>the</td>
<td>agreement</td>
<td>on</td>
<td>the</td>
<td>European</td>
<td>Economic</td>
<td>Area</td>
</tr>
<tr>
<td>L'</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>accord</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>sur</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>la</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>zone</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>économique</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>européenne</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>※色が濃い部分が、AIが翻訳時に注目している単語です。<br />
"Area" → "zone", "European" → "européenne" と、語順が逆転しても正しく対応付いているのが分かります。</p>
<h2>5. 結論：AIが「検索」を手に入れた日</h2>
<p>この手法（RNNsearch）により、モデルは50単語を超える長文でも精度を落とさずに翻訳できるようになりました。なぜなら、<strong>最初からすべての情報を引き渡しておき、必要な時に必要な情報を自動的に取得（Search）する</strong>ようになったからです。</p>
<p>この「クエリ（デコーダの状態）に合わせて、キー（原文）を検索し、バリュー（情報）を取り出す」という構造は、後に<strong>Transformer</strong>へと進化し、現在のChatGPTなどの大規模言語モデルの基礎となっています。</p>
<p>この論文は、単なる翻訳精度の向上だけでなく、ニューラルネットワークに<strong>「注意（Attention）」という認知機能</strong>を実装した、歴史的な転換点だったと言えるでしょう。</p>
<p>前回の記事：BiRNN完全解剖に戻る</p>
<p><a href="https://ai-researcher.hatenablog.com/entry/2025/12/30/162816">ai-researcher.hatenablog.com</a></p>
        </div>
        <div class="footer">
            <p>&copy; 2026 Ayato AI Studio. Generated by AI Agent.</p>
        </div>
    </div>
</body>

</html>