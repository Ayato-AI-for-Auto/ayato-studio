---
title: "【徹底解説】ResNet（Deep Residual Learning）がAIの歴史を変えた理由"
date: "01/05/2026 08:30:00"
---

# 【深層学習の数理】ResNetはなぜ「勾配消失」と「劣化問題」を解決できたのか？

深層学習（Deep Learning）において、「層を深くする」ことは性能向上の鍵です。しかし、2015年にResNetが登場するまで、100層を超えるようなネットワークの学習は不可能とされてきました。

なぜでしょうか？単に計算量が足りなかったからではありません。そこには、ニューラルネットワークの数学的構造に起因する**「勾配消失」**と**「劣化問題」**という2つの大きな壁が存在したからです。

本記事では、AIエンジニアとして基礎から理解するために、これらの現象を数学の最小単位（微分と連鎖律）から解説し、ResNetがどのようにして歴史を変えたのかを紐解きます。

## 1. 基礎知識：ニューラルネットワークの「学習」とは？

問題を理解するために、まず「学習」が数学的に何をしているのかを最小単位で確認しましょう。

**学習の定義：**  
ネットワーク内の膨大なパラメータ（重み $w$）を、出力の誤差 $E$ が最小になるように調整し続ける計算プロセスのこと。

この調整には**「誤差逆伝播法（Backpropagation）」**が使われます。これは、ゴール（出力層）からスタート（入力層）に向かって、「誤差の原因」を遡って特定していく作業です。

*w*new = *w*old − *η* ·  ∂*E* ∂*w*

ここで重要なのが $\frac{\partial E}{\partial w}$（勾配）です。「重み $w$ を少し動かしたとき、誤差 $E$ がどれくらい変わるか」を表します。この値が計算できなければ、重みをどう更新していいかわかりません。

## 2. 勾配消失問題（Vanishing Gradient Problem）の正体

かつて、層を深くするとこの「勾配」が入力層に届く前に消えてしまう（ゼロになる）現象が起きていました。これが勾配消失問題です。

### 2.1 数学的メカニズム：微分の連鎖律（Chain Rule）

誤差逆伝播法は、数学的には**「微分の連鎖律」**の繰り返しです。

ある層の入力を $x$、重みを $w$、出力を $y$ とすると、誤差 $E$ に対する勾配は以下のように積で表されます。

深い層（例えば $n$ 層）がある場合、この「掛け算」が $n$ 回繰り返されます。

入力  層1  層2 ...  層n  出力     × 0.25  × 0.25  ほぼ0! 

図1: 逆伝播における「1より小さい数」の掛け算の繰り返しが勾配を消滅させる。

### 2.2 犯人は「シグモイド関数」の微分特性

ResNet登場以前、活性化関数には生物学的ニューロンに近い「シグモイド関数」がよく使われていました。

しかし、シグモイド関数 $\sigma(x) = \frac{1}{1+e^{-x}}$ の微分係数（接線の傾き）には致命的な弱点があります。

Sigmoid f(x) 微分 f'(x)  最大値 = 0.25 

図2: シグモイド関数（青）とその微分（赤）。微分の最大値はわずか0.25しかない。

**数学的事実：**  
シグモイド関数の微分の最大値は **0.25 ($1/4$)** です。  
つまり、層を1つ遡るごとに、勾配は最大でも **1/4** に減衰します。

これが $n$ 層重なるとどうなるでしょうか？

$$ \text{勾配} \propto (0.25)^n = \left(\frac{1}{4}\right)^n $$

例えば10層でも $(1/4)^{10} \approx 0.00000095$ となり、勾配は事実上消滅します。これが原因で、入力層付近のパラメータはいつまで経っても初期値（ランダム）から変化できず、学習が成立しなかったのです。

*※現在はReLU関数（正の領域で微分が常に1）の使用でこの問題は緩和されましたが、それでも超深層モデルでは次の「劣化問題」が発生します。*

## 3. 劣化問題（Degradation Problem）のパラドックス

勾配消失がある程度解決された後も、研究者たちを悩ませたのが「劣化問題」です。

これは、「層を増やせば増やすほど、学習エラー（訓練誤差）が悪化してしまう」という現象です。過学習とは異なり、訓練データに対してさえ性能が出ないのです。

### 3.1 「恒等写像」すら学習できない難しさ

ここで数学的なパラドックスが生じます。  
もし、ある浅いモデルが最適な性能を出せているなら、それに「何もしない層（恒等写像）」を追加しただけの深いモデルは、少なくとも浅いモデルと同じ性能を出せるはずです。

* **浅いモデル：** $y = H(x)$ （最適）
* **深いモデル：** $y = H(x)$ にさらに層を追加。追加した層が $f(x) = x$ （恒等写像）になれば、全体として $H(x)$ と同じになるはず。

しかし、現実の多層ネットワークにとって、**「入力 $x$ をそのまま出力する（$f(x)=x$）」というパラメータを学習で見つけること**は、想像以上に困難だったのです。非線形な層を何重にも重ねて「何もしない」を実現するのは、数値計算的に非常に不安定で難しいタスクです。

## 4. ResNetの解決策：残差学習の発明

ResNetの核心は、「恒等写像を学習するのが難しいなら、最初から恒等写像を組み込んでしまえばいい」という逆転の発想にあります。

### 4.1 $F(x) = H(x) - x$ を学習する

学習したい関数を $H(x)$ としたとき、ResNetではネットワークに差分（残差）$F(x)$ を学習させます。

$$ H(x) = F(x) + x $$

図で見ると、入力 $x$ が「ショートカット接続」を通って出力に直接加算される形になります。

### 4.2 なぜこれで解決するのか？

**1. 恒等写像の実現が容易：**  
もし最適な変換が「何もしないこと（恒等写像）」だった場合、ResNetでは $F(x)$（重み）をすべてゼロにするだけで済みます。非線形な層で $x$ を再現するより、重みをゼロに近づける（$F(x) \to 0$）ほうが、最適化アルゴリズムにとって圧倒的に簡単です。

**2. 勾配のハイウェイ（+1 の効果）：**  
逆伝播の数式を見ると、その威力がわかります。  
$y = F(x) + x$ を微分すると：

$$ \frac{\partial y}{\partial x} = \frac{\partial F}{\partial x} + 1 $$

この **$+1$** が決定的に重要です。 たとえ複雑な層 $F(x)$ の勾配 $\frac{\partial F}{\partial x}$ が小さく（例えば0.00001に）なっても、勾配全体は $0.00001 + 1 \approx 1$ となります。  
これにより、勾配は減衰することなくショートカットを通って、ネットワークの最下層（入力層）まで直接伝わります。

**結論：**  
ResNetは、ショートカット接続という「勾配の直通ルート」を作ることで、100層、1000層と深くしても、情報が劣化・消失することなく学習できる構造を実現したのです。

---

## 5. 視覚的直感：ResNetは「地形」を滑らかにする

最後に、なぜResNetが学習しやすいのかを、「損失関数の地形（Loss Landscape）」という視点から解説します。実は、この地形の滑らかさは、先ほどセクション4で見た**「勾配のハイウェイ（$+1$）」が生み出す必然的な結果**なのです。

ResNetなし (56層)    多くの局所解・鋭い谷

ResNetあり (56層)     滑らかな凸形状

図3: 損失関数の地形比較（Li et al., 2018に基づく概念図）。  
ResNet（右）は「+1」の効果により地形が滑らかになる。

### 5.1 「+1」が地形を平らにする理由

なぜショートカット接続があるだけで、ここまで地形が変わるのでしょうか？ 再び、先ほどの微分の式を思い出してください。

$$ \frac{\partial y}{\partial x} = 1 + \frac{\partial F}{\partial x} $$

この式における **「1」** は、幾何学的には**「平坦な直線」**を意味します。これをさらに微分して、地形の曲がり具合（曲率）を確認してみましょう。

$$ \text{曲率} \propto \frac{\partial^2 y}{\partial x^2} = \frac{\partial^2}{\partial x^2}(x) + \frac{\partial^2 F}{\partial x^2} = 0 + \frac{\partial^2 F}{\partial x^2} $$

* **直線の曲率はゼロ：** ショートカット項（$x$）は直線なので、どれだけ微分しても曲率（2階微分）は **0** です。
* **ノイズの緩和：** Plain Netでは非線形層 $F(x)$ が複雑に絡み合い、曲率が爆発的に大きくなります（＝鋭い谷ができる）。しかし、ResNetでは常に「曲率ゼロ」の成分がメインストリームとして流れているため、非線形層の暴れ具合が緩和され、全体として地形が平均化（滑らかに）されるのです。

### 5.2 破砕勾配 (Shattered Gradients) の防止

「+1」の経路が存在することで、勾配の相関関係が深層まで保たれます。これにより、少しパラメータが動いただけで勾配の向きがバラバラになる「破砕勾配」現象が防がれ、ボールが転がりやすい滑らかな斜面（Convexに近い形状）が形成されるのです。