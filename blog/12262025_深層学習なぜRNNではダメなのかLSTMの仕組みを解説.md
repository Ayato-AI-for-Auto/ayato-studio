---
title: "【深層学習】なぜRNNではダメなのか？LSTMの仕組みを解説"
date: "12/26/2025 17:45:16"
---

[お題「AI」](https://blog.hatena.ne.jp/-/odai/6802888565255276830)

こんにちは。今回は、時系列データ処理のデファクトスタンダードとして長年君臨してきた **LSTM (Long Short-Term Memory)** について解説します。

「RNNと何が違うの？」「数式が多くて挫折した」という方に向けて、**「2つの回路による分業」**という視点から、その内部構造を視覚的に紐解いていきます。

**目次**

* [人間の思考とRNN](#section-1)
* [RNNが抱える「長期依存」の問題](#section-2)
* [救世主：LSTMの「2つの回路」](#section-3)
* [図解：LSTMの「3つのステップ」](#section-4)
* [まとめ](#section-5)

## 人間の思考とRNN

私たちは毎秒、ゼロから思考を開始しているわけではありません。この記事を読むときも、**前の単語の理解に基づいて、今の単語を理解**しています。

従来のニューラルネットワークには、この「持続性」がありませんでした。その欠点を克服するために生まれたのが **リカレントニューラルネットワーク（RNN）** です。

### RNNのループ構造

RNNは、ループ機能を持つことで情報の永続化を可能にしました。

図の凡例  NN層  演算  RNN x   A  h   =  時間展開  x0   A  h0    x1   A  h1  ...        

図1：RNNは過去の情報を次のステップに引き継ぐ

## RNNが抱える「長期依存」の問題

RNNは直前の情報は覚えられますが、情報が古くなると忘れてしまいます。「空の色は...」のすぐ後なら「青」と予測できますが、長い文章の最後で冒頭の言葉を思い出すのは苦手です。

**RNNの弱点**  
長期的な依存関係を学習しようとすると、勾配消失などが原因でうまくいかない。パラメータ調整が非常にシビア。

## 救世主：LSTMの「2つの回路」

ここが今回の解説の最重要ポイントです。

RNNの問題を解決するために、LSTMは大胆な構造を採用しました。それは、情報の通り道を**「長期記憶用」と「短期記憶用」の2本に分ける**というアイデアです。

① 長期記憶の回路 (Cell State) ② 短期記憶の回路 (Hidden State / RNN的役割)  普段は使われない「記録保持専用」のライン  メインで活動する「作業用」のライン    忘却ゲート 情報を削除     入力ゲート 情報を記録     出力ゲート 情報を読み出し  

図2：LSTMは「2階建て」の構造になっている

### メイン回路とサブ回路の関係

* **短期記憶の回路（下段）**: RNNと同じように、日々のタスクや直近の処理を行います。普段はここだけで作業しています。
* **長期記憶の回路（上段）**: 重要な情報だけをずっと保持しておく「アーカイブ」です。普段は何もしません。

短期記憶回路が「あ、これは大事だ！」と思ったときだけ長期回路に書き込み（入力ゲート）、逆に「昔の情報が必要だ！」と思ったときだけ長期回路から情報を引き出す（出力ゲート）のです。

これにより、**「普段は長期記憶に余計な干渉をしない」**ことが保証され、情報の劣化（勾配消失）を防ぐことができるのです。

### 核となる概念：セルの状態（Cell State）

この長期記憶の回路（Cell State）は、しばしば**「ベルトコンベア」**に例えられます。

セルの状態（長期記憶ライン） 情報は変更されずに一直線に流れていく  ×  + 忘却ゲートからの干渉  入力ゲートからの干渉  

図3：ベルトコンベア上を情報は劣化せずに流れる

## 図解：LSTMの「3つのステップ」

では、短期回路（メイン）が長期回路（アーカイブ）に対してどのようにアクセスしているか、3つのゲートの役割を見ていきましょう。

### Step 1: 忘却ゲート（いらない情報を捨てる）

「前の状態 ![h_{t-1}](https://chart.apis.google.com/chart?cht=tx&chl=h_%7Bt-1%7D)」と「今の入力 ![x_t](https://chart.apis.google.com/chart?cht=tx&chl=x_t)」を見て、**何を忘れるか（0〜1）** を決めます。

ht-1 xt    σ 忘却ゲート層    × 0 = 完全に忘れる 1 = 覚えている 

図4：忘却ゲートの処理

### Step 2: 入力ゲート（新しい情報を足す）

どの情報を更新するかを決め（シグモイド）、新しい候補値を作成（tanh）し、それらをセル状態に追加します。

ht-1 xt        σ 入力ゲート  tanh 候補値     ×      + 新しい情報を追加！ 

図5：入力ゲートの処理

### Step 3: 出力ゲート（次の隠れ状態を作る）

最後に、セル状態をtanhに通して正規化し、出力したい部分だけをシグモイドゲートでフィルタリングして出力します。

h\_t = o\_t \* tanh(C\_t)

## まとめ

LSTMは一見複雑に見えますが、その本質は**「普段使いの回路（RNN）」と「保存用の回路（Cell）」を分けたこと**にあります。

* 普段は短期記憶（RNN回路）で処理を回す。
* 必要な時だけ、長期記憶（Cell回路）に「書き込み請求」や「読み出し請求」を行う。

この分業体制こそが、LSTMが長期依存問題を解決できた最大の理由なのです。

---

Reference:  
[Understanding LSTM Networks by Christopher Olah](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)