<!DOCTYPE html>
<html lang="ja">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【徹底解説】ResNet（Deep Residual Learning）がAIの歴史を変えた理由 | Ayato Studio</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;700;900&family=Noto+Sans+JP:wght@400;700&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="../style.css">
    <style>
        /* Blog specific overrides */
        body {
            background-color: #0a0a0a;
            color: #e0e0e0;
        }

        .blog-container {
            max-width: 800px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        .blog-header {
            margin-bottom: 40px;
            border-bottom: 1px solid #333;
            padding-bottom: 20px;
        }

        .blog-header h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
            background: linear-gradient(45deg, #fff, #888);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .blog-meta {
            color: #888;
            font-size: 0.9rem;
        }

        .blog-content {
            font-size: 1.1rem;
            line-height: 1.8;
            color: #d0d0d0;
        }

        .blog-content h2 {
            margin-top: 40px;
            margin-bottom: 20px;
            color: #fff;
            border-left: 4px solid #fff;
            padding-left: 15px;
        }

        .blog-content h3 {
            margin-top: 30px;
            margin-bottom: 15px;
            color: #ddd;
        }

        .blog-content p {
            margin-bottom: 20px;
        }

        .blog-content img {
            max-width: 100%;
            border-radius: 8px;
            margin: 20px 0;
        }

        .blog-content pre {
            background: #1a1a1a;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            border: 1px solid #333;
        }

        .back-link {
            display: inline-block;
            margin-top: 40px;
            color: #888;
            text-decoration: none;
            transition: color 0.3s;
        }

        .back-link:hover {
            color: #fff;
        }

        /* Hatena specific cleanups */
        .keyword {
            text-decoration: none;
            color: inherit;
            pointer-events: none;
        }
    </style>
</head>

<body>
    <div class="blog-container">
        <a href="../index.html" class="back-link">← Back to Home</a>

        <header class="blog-header">
            <h1>【徹底解説】ResNet（Deep Residual Learning）がAIの歴史を変えた理由</h1>
            <div class="blog-meta">
                <span>Published: 01/05/2026 08:30:00</span>
            </div>
        </header>

        <article class="blog-content">
            <p> </p>

            <p>
                <script>
                    window.MathJax = {
                        tex: {
                            inlineMath: [['$', '$'], ['\\(', '\\)']],
                            displayMath: [['$$', '$$'], ['\\[', '\\]']],
                            processEscapes: true
                        },
                        svg: {
                            fontCache: 'global'
                        }
                    };
                </script>
                <script id="MathJax-script" async=""
                    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
            </p>
            <style>
                :root {
                    --primary-color: #2c3e50;
                    --accent-color: #2980b9;
                    --bg-color: #fdfdfd;
                    --text-color: #333;
                    --code-bg: #f4f6f8;
                    --border-color: #e1e4e8;
                    --diagram-bg: #fff;
                }

                body {
                    font-family: "Helvetica Neue", Arial, "Hiragino Kaku Gothic ProN", "Hiragino Sans", Meiryo, sans-serif;
                    line-height: 1.8;
                    color: var(--text-color);
                    background-color: var(--bg-color);
                    max-width: 900px;
                    margin: 0 auto;
                    padding: 40px 20px;
                }

                h1 {
                    font-size: 2.0rem;
                    border-bottom: 2px solid var(--accent-color);
                    padding-bottom: 10px;
                    margin-bottom: 30px;
                    color: var(--primary-color);
                    line-height: 1.4;
                }

                h2 {
                    font-size: 1.6rem;
                    margin-top: 50px;
                    margin-bottom: 25px;
                    padding-left: 15px;
                    border-left: 5px solid var(--accent-color);
                    color: var(--primary-color);
                    background: #f1f8ff;
                    padding: 10px 15px;
                    border-radius: 0 5px 5px 0;
                }

                h3 {
                    font-size: 1.3rem;
                    margin-top: 35px;
                    color: var(--primary-color);
                    border-bottom: 1px solid #ddd;
                    padding-bottom: 5px;
                }

                p {
                    margin-bottom: 1.5em;
                    text-align: justify;
                }

                ul,
                ol {
                    margin-bottom: 1.5em;
                    padding-left: 20px;
                }

                li {
                    margin-bottom: 0.5em;
                }

                /* Highlight Boxes */
                .highlight-box {
                    background-color: #fff8e1;
                    border-left: 4px solid #f39c12;
                    padding: 15px;
                    margin: 20px 0;
                    border-radius: 4px;
                    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
                }

                .info-box {
                    background-color: #e8f4fd;
                    border-left: 4px solid #3498db;
                    padding: 15px;
                    margin: 20px 0;
                    border-radius: 4px;
                }

                /* Math & Code */
                .math-block {
                    background-color: #fff;
                    padding: 20px;
                    text-align: center;
                    font-size: 1.1rem;
                    border: 1px solid var(--border-color);
                    border-radius: 5px;
                    margin: 20px 0;
                    overflow-x: auto;
                    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.05);
                }

                pre {
                    background-color: #282c34;
                    color: #abb2bf;
                    padding: 20px;
                    border-radius: 8px;
                    overflow-x: auto;
                    font-family: Consolas, Monaco, monospace;
                    font-size: 0.9rem;
                    line-height: 1.5;
                    margin-bottom: 2em;
                }

                code {
                    font-family: Consolas, monospace;
                    background: #eee;
                    padding: 2px 4px;
                    border-radius: 3px;
                }

                .python-keyword {
                    color: #c678dd;
                }

                .python-def {
                    color: #61afef;
                }

                .python-class {
                    color: #e5c07b;
                }

                .python-comment {
                    color: #5c6370;
                    font-style: italic;
                }

                /* Diagrams */
                .diagram-container {
                    display: flex;
                    flex-direction: column;
                    align-items: center;
                    background-color: var(--diagram-bg);
                    border: 1px solid #ddd;
                    border-radius: 8px;
                    padding: 30px;
                    margin: 30px 0 10px 0;
                    overflow-x: auto;
                }

                .caption {
                    font-size: 0.85rem;
                    color: #666;
                    text-align: center;
                    margin-top: 10px;
                    margin-bottom: 30px;
                    font-style: italic;
                }

                /* Gradient Vanishing Diagram */
                .gv-svg {
                    width: 100%;
                    max-width: 600px;
                    height: 250px;
                }

                .node {
                    fill: #3498db;
                    stroke: #2980b9;
                    stroke-width: 2;
                }

                .node-text {
                    fill: white;
                    font-size: 14px;
                    text-anchor: middle;
                    dominant-baseline: central;
                }

                .arrow {
                    stroke: #333;
                    stroke-width: 2;
                    marker-end: url(#arrowhead);
                }

                .back-arrow {
                    stroke: #e74c3c;
                    stroke-width: 2;
                    stroke-dasharray: 4, 4;
                    marker-end: url(#arrowhead-red);
                }

                .label-text {
                    font-size: 12px;
                    fill: #555;
                }

                /* Sigmoid Graph */
                .sigmoid-svg {
                    width: 100%;
                    max-width: 500px;
                    height: 250px;
                    background: #fff;
                }

                .axis {
                    stroke: #333;
                    stroke-width: 1;
                }

                .curve-sigmoid {
                    fill: none;
                    stroke: #3498db;
                    stroke-width: 3;
                }

                .curve-derivative {
                    fill: none;
                    stroke: #e74c3c;
                    stroke-width: 2;
                    stroke-dasharray: 5, 3;
                }

                .grid-line {
                    stroke: #eee;
                    stroke-width: 1;
                }
            </style>
            <h1 id="深層学習の数理ResNetはなぜ勾配消失と劣化問題を解決できたのか">【深層学習の数理】ResNetはなぜ「勾配消失」と「劣化問題」を解決できたのか？</h1>
            <p>深層学習（<a class="keyword" href="https://d.hatena.ne.jp/keyword/Deep%20Learning">Deep
                    Learning</a>）において、「層を深くする」ことは性能向上の鍵です。しかし、2015年にResNetが登場するまで、100層を超えるようなネットワークの学習は不可能とされてきました。</p>
            <p>なぜでしょうか？単に計算量が足りなかったからではありません。そこには、<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%A5%CB%A5%E5%A1%BC%A5%E9%A5%EB%A5%CD%A5%C3%A5%C8%A5%EF%A1%BC%A5%AF">ニューラルネットワーク</a>の数学的構造に起因する<strong>「勾配消失」</strong>と<strong>「劣化問題」</strong>という2つの大きな壁が存在したからです。
            </p>
            <p>本記事では、AIエンジニアとして基礎から理解するために、これらの現象を数学の最小単位（<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%C8%F9%CA%AC">微分</a>と連鎖律）から解説し、ResNetがどのようにして歴史を変えたのかを紐解きます。
            </p>
            <h2 id="1-基礎知識ニューラルネットワークの学習とは">1. 基礎知識：<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%A5%CB%A5%E5%A1%BC%A5%E9%A5%EB%A5%CD%A5%C3%A5%C8%A5%EF%A1%BC%A5%AF">ニューラルネットワーク</a>の「学習」とは？
            </h2>
            <p>問題を理解するために、まず「学習」が数学的に何をしているのかを最小単位で確認しましょう。</p>
            <div class="info-box"><strong>学習の定義：</strong><br />ネットワーク内の膨大なパラメータ（重み $w$）を、出力の誤差 $E$
                が最小になるように調整し続ける計算プロセスのこと。</div>
            <p>この調整には<strong>「<a class="keyword"
                        href="https://d.hatena.ne.jp/keyword/%B8%ED%BA%B9%B5%D5%C5%C1%C7%C5">誤差逆伝播</a>法（Backpropagation）」</strong>が使われます。これは、ゴール（出力層）からスタート（入力層）に向かって、「誤差の原因」を遡って特定していく作業です。
            </p>
            <div
                style="background-color: #fff; padding: 20px; text-align: center; font-size: 1.2rem; border: 1px solid #e1e4e8; border-radius: 5px; margin: 20px 0; font-family: 'Times New Roman', serif;">
                <em>w</em><sub>new</sub> = <em>w</em><sub>old</sub> − <em>η</em> · <span
                    style="display: inline-block; vertical-align: middle; text-align: center; margin: 0 5px;"> <span
                        style="display: block; border-bottom: 1px solid #333; padding: 0 5px; line-height: 1.2;">∂<em>E</em></span>
                    <span style="display: block; padding: 0 5px; line-height: 1.2;">∂<em>w</em></span> </span></div>
            <p>ここで重要なのが $\frac{\partial E}{\partial w}$（勾配）です。「重み $w$ を少し動かしたとき、誤差 $E$
                がどれくらい変わるか」を表します。この値が計算できなければ、重みをどう更新していいかわかりません。</p>
            <h2 id="2-勾配消失問題Vanishing-Gradient-Problemの正体">2. 勾配消失問題（Vanishing Gradient Problem）の正体</h2>
            <p>かつて、層を深くするとこの「勾配」が入力層に届く前に消えてしまう（ゼロになる）現象が起きていました。これが勾配消失問題です。</p>
            <h3 id="21-数学的メカニズム微分の連鎖律Chain-Rule">2.1 数学的メカニズム：<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%C8%F9%CA%AC">微分</a>の連鎖律（Chain Rule）</h3>
            <p><a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%B8%ED%BA%B9%B5%D5%C5%C1%C7%C5">誤差逆伝播</a>法は、数学的には<strong>「<a
                        class="keyword" href="https://d.hatena.ne.jp/keyword/%C8%F9%CA%AC">微分</a>の連鎖律」</strong>の繰り返しです。
            </p>
            <p>ある層の入力を $x$、重みを $w$、出力を $y$ とすると、誤差 $E$ に対する勾配は以下のように積で表されます。</p>
            <p>
                <script>window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true }, svg: { fontCache: 'global' } };</script>
                <script id="MathJax-script" async=""
                    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
            </p>
            <p>深い層（例えば $n$ 層）がある場合、この「掛け算」が $n$ 回繰り返されます。</p>
            <div class="diagram-container"><svg class="gv-svg" viewbox="0 0 600 200">
                    <defs>
                        <marker id="arrowhead" markerwidth="10" markerheight="7" refx="9" refy="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" fill="#333"></polygon>
                        </marker>
                        <marker id="arrowhead-red" markerwidth="10" markerheight="7" refx="9" refy="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" fill="#e74c3c"></polygon>
                        </marker>
                    </defs>
                    <circle cx="50" cy="100" r="25" class="node"></circle> <text x="50" y="100"
                        class="node-text">入力</text>
                    <circle cx="150" cy="100" r="25" class="node"></circle> <text x="150" y="100"
                        class="node-text">層1</text>
                    <circle cx="250" cy="100" r="25" class="node"></circle> <text x="250" y="100"
                        class="node-text">層2</text> <text x="350" y="100" font-size="24" fill="#333">...</text>
                    <circle cx="450" cy="100" r="25" class="node"></circle> <text x="450" y="100"
                        class="node-text">層n</text>
                    <circle cx="550" cy="100" r="25" class="node" style="fill: #2ecc71; stroke: #27ae60;"></circle>
                    <text x="550" y="100" class="node-text">出力</text>
                    <line x1="75" y1="80" x2="125" y2="80" class="arrow"></line>
                    <line x1="175" y1="80" x2="225" y2="80" class="arrow"></line>
                    <line x1="475" y1="80" x2="525" y2="80" class="arrow"></line>
                    <path d="M 525 120 L 475 120" class="back-arrow"></path> <text x="500" y="140" class="label-text"
                        text-anchor="middle">× 0.25</text>
                    <path d="M 225 120 L 175 120" class="back-arrow"></path> <text x="200" y="140" class="label-text"
                        text-anchor="middle">× 0.25</text>
                    <path d="M 125 120 L 75 120" class="back-arrow"></path> <text x="100" y="155" class="label-text"
                        text-anchor="middle" fill="#e74c3c" font-weight="bold">ほぼ0!</text>
                </svg>
                <div class="caption">図1: 逆伝播における「1より小さい数」の掛け算の繰り返しが勾配を消滅させる。</div>
            </div>
            <h3 id="22-犯人はシグモイド関数の微分特性">2.2 犯人は「<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%A5%B7%A5%B0%A5%E2%A5%A4%A5%C9%B4%D8%BF%F4">シグモイド関数</a>」の<a
                    class="keyword" href="https://d.hatena.ne.jp/keyword/%C8%F9%CA%AC">微分</a>特性</h3>
            <p>ResNet登場以前、活性化関数には生物学的<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%A5%CB%A5%E5%A1%BC%A5%ED%A5%F3">ニューロン</a>に近い「<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%A5%B7%A5%B0%A5%E2%A5%A4%A5%C9%B4%D8%BF%F4">シグモイド関数</a>」がよく使われていました。
            </p>
            <p>しかし、<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%A5%B7%A5%B0%A5%E2%A5%A4%A5%C9%B4%D8%BF%F4">シグモイド関数</a> $\<a
                    class="keyword" href="https://d.hatena.ne.jp/keyword/sigma">sigma</a>(x) = \frac{1}{1+e^{-x}}$ の<a
                    class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%C8%F9%CA%AC%B7%B8%BF%F4">微分係数</a>（接線の傾き）には致命的な弱点があります。</p>
            <div class="diagram-container"><svg class="sigmoid-svg" viewbox="0 0 400 200">
                    <line x1="200" y1="0" x2="200" y2="200" class="axis"></line>
                    <line x1="0" y1="180" x2="400" y2="180" class="axis"></line>
                    <path d="M 0 175 C 100 175, 150 170, 200 100 C 250 30, 300 25, 400 25" class="curve-sigmoid"></path>
                    <path d="M 0 178 Q 200 80, 400 178" class="curve-derivative"></path> <text x="280" y="50"
                        fill="#3498db" font-weight="bold">Sigmoid f(x)</text> <text x="280" y="160" fill="#e74c3c"
                        font-weight="bold"><a class="keyword" href="https://d.hatena.ne.jp/keyword/%C8%F9%CA%AC">微分</a>
                        f'(x)</text>
                    <line x1="200" y1="180" x2="200" y2="120" stroke="#aaa" stroke-dasharray="2,2"></line> <text x="210"
                        y="120" fill="#333" font-size="12">最大値 = 0.25</text>
                </svg>
                <div class="caption">図2: <a class="keyword"
                        href="https://d.hatena.ne.jp/keyword/%A5%B7%A5%B0%A5%E2%A5%A4%A5%C9%B4%D8%BF%F4">シグモイド関数</a>（青）とその<a
                        class="keyword" href="https://d.hatena.ne.jp/keyword/%C8%F9%CA%AC">微分</a>（赤）。<a class="keyword"
                        href="https://d.hatena.ne.jp/keyword/%C8%F9%CA%AC">微分</a>の最大値はわずか0.25しかない。</div>
            </div>
            <div class="highlight-box"><strong>数学的事実：</strong><br /><a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%A5%B7%A5%B0%A5%E2%A5%A4%A5%C9%B4%D8%BF%F4">シグモイド関数</a>の<a
                    class="keyword" href="https://d.hatena.ne.jp/keyword/%C8%F9%CA%AC">微分</a>の最大値は <strong>0.25
                    ($1/4$)</strong> です。<br />つまり、層を1つ遡るごとに、勾配は最大でも <strong>1/4</strong> に減衰します。</div>
            <p>これが $n$ 層重なるとどうなるでしょうか？</p>
            <div class="math-block">$$ \text{勾配} \propto (0.25)^n = \left(\frac{1}{4}\right)^n $$</div>
            <p>例えば10層でも $(1/4)^{10} \approx 0.00000095$
                となり、勾配は事実上消滅します。これが原因で、入力層付近のパラメータはいつまで経っても初期値（ランダム）から変化できず、学習が成立しなかったのです。</p>
            <p><em>※現在はReLU関数（正の領域で<a class="keyword"
                        href="https://d.hatena.ne.jp/keyword/%C8%F9%CA%AC">微分</a>が常に1）の使用でこの問題は緩和されましたが、それでも超深層モデルでは次の「劣化問題」が発生します。</em>
            </p>
            <h2 id="3-劣化問題Degradation-Problemのパラドックス">3. 劣化問題（Degradation Problem）の<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%A5%D1%A5%E9%A5%C9%A5%C3%A5%AF%A5%B9">パラドックス</a></h2>
            <p>勾配消失がある程度解決された後も、研究者たちを悩ませたのが「劣化問題」です。</p>
            <p>これは、「層を増やせば増やすほど、学習エラー（訓練誤差）が悪化してしまう」という現象です。<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%B2%E1%B3%D8%BD%AC">過学習</a>とは異なり、訓練データに対してさえ性能が出ないのです。</p>
            <h3 id="31-恒等写像すら学習できない難しさ">3.1 「恒等<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%BC%CC%C1%FC">写像</a>」すら学習できない難しさ</h3>
            <p>ここで数学的な<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%A5%D1%A5%E9%A5%C9%A5%C3%A5%AF%A5%B9">パラドックス</a>が生じます。<br />もし、ある浅いモデルが最適な性能を出せているなら、それに「何もしない層（恒等<a
                    class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%BC%CC%C1%FC">写像</a>）」を追加しただけの深いモデルは、少なくとも浅いモデルと同じ性能を出せるはずです。
            </p>
            <ul>
                <li><strong>浅いモデル：</strong> $y = H(x)$ （最適）</li>
                <li><strong>深いモデル：</strong> $y = H(x)$ にさらに層を追加。追加した層が $f(x) = x$ （恒等<a class="keyword"
                        href="https://d.hatena.ne.jp/keyword/%BC%CC%C1%FC">写像</a>）になれば、全体として $H(x)$ と同じになるはず。</li>
            </ul>
            <p>しかし、現実の多層ネットワークにとって、<strong>「入力 $x$ をそのまま出力する（$f(x)=x$）」というパラメータを学習で見つけること</strong>は、想像以上に困難だったのです。<a
                    class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%C8%F3%C0%FE%B7%C1">非線形</a>な層を何重にも重ねて「何もしない」を実現するのは、<a
                    class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%BF%F4%C3%CD%B7%D7%BB%BB">数値計算</a>的に非常に不安定で難しいタスクです。</p>
            <h2 id="4-ResNetの解決策残差学習の発明">4. ResNetの解決策：残差学習の発明</h2>
            <p>ResNetの核心は、「恒等<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%BC%CC%C1%FC">写像</a>を学習するのが難しいなら、最初から恒等<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%BC%CC%C1%FC">写像</a>を組み込んでしまえばいい」という逆転の発想にあります。</p>
            <h3 id="41-Fx--Hx---x-を学習する">4.1 $F(x) = H(x) - x$ を学習する</h3>
            <p>学習したい関数を $H(x)$ としたとき、ResNetではネットワークに差分（残差）$F(x)$ を学習させます。</p>
            <div class="math-block">$$ H(x) = F(x) + x $$</div>
            <p>図で見ると、入力 $x$ が「ショートカット接続」を通って出力に直接加算される形になります。</p>
            <h3 id="42-なぜこれで解決するのか">4.2 なぜこれで解決するのか？</h3>
            <p><strong>1. 恒等<a class="keyword"
                        href="https://d.hatena.ne.jp/keyword/%BC%CC%C1%FC">写像</a>の実現が容易：</strong><br />もし最適な変換が「何もしないこと（恒等<a
                    class="keyword" href="https://d.hatena.ne.jp/keyword/%BC%CC%C1%FC">写像</a>）」だった場合、ResNetでは
                $F(x)$（重み）をすべてゼロにするだけで済みます。<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%C8%F3%C0%FE%B7%C1">非線形</a>な層で $x$ を再現するより、重みをゼロに近づける（$F(x) \to
                0$）ほうが、最適化<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%A5%A2%A5%EB%A5%B4%A5%EA%A5%BA%A5%E0">アルゴリズム</a>にとって圧倒的に簡単です。
            </p>
            <p><strong>2. 勾配のハイウェイ（+1 の効果）：</strong><br />逆伝播の数式を見ると、その威力がわかります。<br />$y = F(x) + x$ を<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%C8%F9%CA%AC">微分</a>すると：</p>
            <div class="math-block">$$ \frac{\partial y}{\partial x} = \frac{\partial F}{\partial x} + 1 $$</div>
            <p>この <strong>$+1$</strong> が決定的に重要です。 たとえ複雑な層 $F(x)$ の勾配 $\frac{\partial F}{\partial x}$
                が小さく（例えば0.00001に）なっても、勾配全体は $0.00001 + 1 \approx 1$
                となります。<br />これにより、勾配は減衰することなくショートカットを通って、ネットワークの最下層（入力層）まで直接伝わります。</p>
            <div class="highlight-box">
                <strong>結論：</strong><br />ResNetは、ショートカット接続という「勾配の直通ルート」を作ることで、100層、1000層と深くしても、情報が劣化・消失することなく学習できる構造を実現したのです。
            </div>
            <hr />
            <h2 id="5-視覚的直感ResNetは地形を滑らかにする">5. 視覚的直感：ResNetは「地形」を滑らかにする</h2>
            <p>最後に、なぜResNetが学習しやすいのかを、「損失関数の地形（Loss
                Landscape）」という視点から解説します。実は、この地形の滑らかさは、先ほどセクション4で見た<strong>「勾配のハイウェイ（$+1$）」が生み出す必然的な結果</strong>なのです。</p>
            <div class="diagram-container" style="flex-direction: row; gap: 20px; flex-wrap: wrap;">
                <div style="flex: 1; min-width: 280px; text-align: center;"><svg viewbox="0 0 300 200"
                        style="width: 100%; border: 1px solid #eee; border-radius: 8px;">
                        <defs>
                            <lineargradient id="grad-rugged" x1="0%" y1="0%" x2="0%" y2="100%">
                                <stop offset="0%" style="stop-color: #ff9a9e; stop-opacity: 1;"></stop>
                                <stop offset="100%" style="stop-color: #fad0c4; stop-opacity: 1;"></stop>
                            </lineargradient>
                        </defs> <text x="150" y="30" font-weight="bold" fill="#e74c3c" text-anchor="middle">ResNetなし
                            (56層)</text>
                        <path d="M 20 120 L 150 60 L 280 120" stroke="#ddd" fill="none"></path>
                        <path d="M 150 60 L 150 180" stroke="#ddd" fill="none" stroke-dasharray="4,4"></path>
                        <path
                            d="M 20 120 L 40 80 L 50 130 L 70 60 L 90 140 L 110 50 L 130 150 L 150 40 L 170 140 L 190 60 L 210 130 L 230 70 L 250 120 L 280 120"
                            fill="url(#grad-rugged)" stroke="#c0392b" stroke-width="2" opacity="0.8"></path> <text
                            x="150" y="180" font-size="12" fill="#555" text-anchor="middle">多くの局所解・鋭い谷</text>
                    </svg></div>
                <div style="flex: 1; min-width: 280px; text-align: center;"><svg viewbox="0 0 300 200"
                        style="width: 100%; border: 1px solid #eee; border-radius: 8px;">
                        <defs>
                            <lineargradient id="grad-smooth" x1="0%" y1="0%" x2="0%" y2="100%">
                                <stop offset="0%" style="stop-color: #a18cd1; stop-opacity: 1;"></stop>
                                <stop offset="100%" style="stop-color: #fbc2eb; stop-opacity: 1;"></stop>
                            </lineargradient>
                        </defs> <text x="150" y="30" font-weight="bold" fill="#8e44ad" text-anchor="middle">ResNetあり
                            (56層)</text>
                        <path d="M 20 120 L 150 60 L 280 120" stroke="#ddd" fill="none"></path>
                        <path d="M 150 60 L 150 180" stroke="#ddd" fill="none" stroke-dasharray="4,4"></path>
                        <path d="M 20 120 Q 150 180 280 120" fill="none" stroke="#ddd" stroke-width="2"></path>
                        <path d="M 20 120 Q 150 240 280 120" fill="url(#grad-smooth)" stroke="#8e44ad" stroke-width="3">
                        </path> <text x="150" y="180" font-size="12" fill="#555" text-anchor="middle">滑らかな凸形状</text>
                    </svg></div>
            </div>
            <div class="caption">図3: 損失関数の地形比較（Li et al., 2018に基づく概念図）。<br />ResNet（右）は「+1」の効果により地形が滑らかになる。</div>
            <h3 id="51-1が地形を平らにする理由">5.1 「+1」が地形を平らにする理由</h3>
            <p>なぜショートカット接続があるだけで、ここまで地形が変わるのでしょうか？ 再び、先ほどの<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%C8%F9%CA%AC">微分</a>の式を思い出してください。</p>
            <div class="math-block">$$ \frac{\partial y}{\partial x} = 1 + \frac{\partial F}{\partial x} $$</div>
            <p>この式における <strong>「1」</strong> は、<a class="keyword"
                    href="https://d.hatena.ne.jp/keyword/%B4%F6%B2%BF%B3%D8">幾何学</a>的には<strong>「平坦な直線」</strong>を意味します。これをさらに<a
                    class="keyword" href="https://d.hatena.ne.jp/keyword/%C8%F9%CA%AC">微分</a>して、地形の曲がり具合（曲率）を確認してみましょう。
            </p>
            <div class="math-block">$$ \text{曲率} \propto \frac{\partial^2 y}{\partial x^2} = \frac{\partial^2}{\partial
                x^2}(x) + \frac{\partial^2 F}{\partial x^2} = 0 + \frac{\partial^2 F}{\partial x^2} $$</div>
            <ul>
                <li><strong>直線の曲率はゼロ：</strong> ショートカット項（$x$）は直線なので、どれだけ<a class="keyword"
                        href="https://d.hatena.ne.jp/keyword/%C8%F9%CA%AC">微分</a>しても曲率（2階<a class="keyword"
                        href="https://d.hatena.ne.jp/keyword/%C8%F9%CA%AC">微分</a>）は <strong>0</strong> です。</li>
                <li><strong>ノイズの緩和：</strong> Plain Netでは<a class="keyword"
                        href="https://d.hatena.ne.jp/keyword/%C8%F3%C0%FE%B7%C1">非線形</a>層 $F(x)$
                    が複雑に絡み合い、曲率が爆発的に大きくなります（＝鋭い谷ができる）。しかし、ResNetでは常に「曲率ゼロ」の成分がメインストリームとして流れているため、<a class="keyword"
                        href="https://d.hatena.ne.jp/keyword/%C8%F3%C0%FE%B7%C1">非線形</a>層の暴れ具合が緩和され、全体として地形が平均化（滑らかに）されるのです。
                </li>
            </ul>
            <h3 id="52-破砕勾配-Shattered-Gradients-の防止">5.2 破砕勾配 (Shattered Gradients) の防止</h3>
            <p>「+1」の経路が存在することで、勾配の相関関係が深層まで保たれます。これにより、少しパラメータが動いただけで勾配の向きがバラバラになる「破砕勾配」現象が防がれ、ボールが転がりやすい滑らかな斜面（Convexに近い形状）が形成されるのです。
            </p>
        </article>

        <a href="../index.html" class="back-link">← Back to Home</a>
    </div>
    <script src="../config.js"></script>
    <script src="../script.js"></script>
</body>

</html>