---
title: "【衝撃】プロンプトを2回繰り返すだけでLLMの性能が劇的向上？「Prompt Repetition」論文解説"
date: "12/27/2025 20:47:48"
---

TechInsider AI論文解説

 [元論文を読む](https://arxiv.org/abs/2512.14982)

Prompt Engineering

# 「推論」はもう不要？ プロンプトを2回繰り返すだけで LLMの性能が爆上がりする件

2025.12.27 5分で読めます

## この記事の要点（TL;DR）

* 最新論文により、**プロンプトを単に2回繰り返すだけ**でGeminiやGPTの回答精度が向上することが判明。
* CoT（思考の連鎖）を使わないため、**コスト（出力トークン）も待ち時間（レイテンシ）も増えない**。
* 因果的モデル（GPT）で、擬似的に双方向モデル（BERT）のような**「深い文脈理解」**を強制するハック手法。

## 1 CoT（思考の連鎖）のジレンマ

「LLMの精度を上げたいなら、『ステップバイステップで考えて』と指示しろ」。これはプロンプトエンジニアリングの常識でした。 しかし、APIを利用してアプリケーションを開発するエンジニアにとって、これは\*\*「コスト」と「速度」のトレードオフ\*\*を意味します。

### CoTありの場合

「答えはAです。なぜなら〜」と思考過程を出力するため、**トークン課金が増え、レスポンスも遅い**。

### 今回の手法

「答えはAです。」と即答させるが、精度は高い。**安くて速い**。

今回紹介する論文**「Prompt Repetition Improves Non-Reasoning LLMs」**は、このジレンマを解決する驚くべき発見を報告しています。

## 2 魔法の呪文「<Query><Query>」

手法は拍子抜けするほど簡単です。LLMに入力するプロンプト（質問）を、**ただ2回繰り返して送信するだけ**です。

Input Prompt Example  Copy

`<コンテキスト>  
以下の文章を読んで質問に答えてください...（文章）...  
  
<質問>  
この文章の結論は何ですか？  
  
<コンテキスト（繰り返し）>  
以下の文章を読んで質問に答えてください...（文章）...  
  
<質問（繰り返し）>  
この文章の結論は何ですか？`

推論（Reasoning）モードを使わず、この「繰り返しプロンプト」を入力するだけで、Gemini、GPT-4o、Claude、DeepSeekといった主要モデルすべてでパフォーマンスが向上することが確認されました。

## 3 なぜこれだけで賢くなるのか？

「2回言われるとよく分かる」というのは人間と同じですが、LLMの場合はより技術的な理由があります。それは**「因果的注意機構（Causal Attention）」のハック**です。

### 💡 技術的解説：擬似的な双方向エンコーダ化

現在のLLM（GPTなど）は、原則として「前の単語」しか見ることができません（Causal Masking）。  
しかし、入力を2回繰り返すとどうなるでしょうか？

1回目のクエリ処理  
未来が見えない（片方向）

2回目のクエリ処理  
1回目の全内容が見える！  
（実質、双方向）

これにより、モデルは推論時間をかけずに、BERTのような**「文章全体を俯瞰した深い理解」**を強制的に獲得できるのです。

## 4 Pythonユーザーへの福音

Web版のChatGPTやGeminiを使っている場合、すでに裏側でこのような最適化が行われている（あるいは今後実装される）可能性があります。 しかし、**PythonなどでAPIを直接叩いている開発者**にとっては、この手法は即戦力です。

* #### コスト削減

  入力トークンは増えますが、高価な出力トークンは最小限（回答のみ）で済みます。
* #### 超低レイテンシ

  「プレフィル（入力処理）」は並列処理されるため、入力が倍になっても待ち時間はほぼ変わりません。

## まとめ：明日から使える「無料のパワーアップ」

「複雑な推論はさせたくないが、精度は落としたくない」。  
そんなワガママな要件があるときは、迷わずプロンプトをループさせてみてください。  
LLMは2度聞かれると、私たちが思う以上に深く理解してくれます。

 [論文全文を読む (arXiv)](https://arxiv.org/abs/2512.14982)

© 2025 TechInsider Blog. Based on the research "Prompt Repetition Improves Non-Reasoning LLMs".

※本記事は論文内容の解説であり、特定の手法による利益を保証するものではありません。