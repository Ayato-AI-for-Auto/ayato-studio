<!DOCTYPE html>
<html lang="ja">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Attention Is All You Need 解説 | Ayato Studio</title>
    <meta name="description" content="TransformerモデルとAttention機構の徹底解説。AIエンジニアAyatoによる技術考察。">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;700;900&family=Noto+Sans+JP:wght@400;700&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="../style.css">
    <style>
        .article-container {
            max-width: 800px;
            margin: 0 auto;
            padding: 8rem 2rem 4rem;
        }

        .article-header {
            margin-bottom: 4rem;
        }

        .article-title {
            font-size: 2.5rem;
            margin-bottom: 1rem;
            line-height: 1.3;
        }

        .article-meta {
            font-size: 0.9rem;
            opacity: 0.6;
        }

        .article-content {
            line-height: 1.8;
            font-size: 1.05rem;
        }

        .article-content h2 {
            margin-top: 4rem;
            margin-bottom: 2rem;
            font-size: 1.8rem;
            border-bottom: 1px solid var(--glass-border);
            padding-bottom: 0.5rem;
        }

        .article-content h3 {
            margin-top: 3rem;
            margin-bottom: 1.5rem;
            font-size: 1.4rem;
            color: var(--accent-color);
        }

        .article-content p {
            margin-bottom: 1.5rem;
        }

        .article-content ul {
            margin-bottom: 1.5rem;
            padding-left: 1.5rem;
        }

        .article-content li {
            margin-bottom: 0.5rem;
        }

        .article-content code {
            background: rgba(255, 255, 255, 0.1);
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: monospace;
        }

        .article-content blockquote {
            border-left: 4px solid var(--secondary-color);
            padding-left: 1.5rem;
            margin: 2rem 0;
            opacity: 0.8;
            font-style: italic;
        }

        .back-link {
            display: inline-block;
            margin-top: 4rem;
            color: var(--accent-color);
            text-decoration: none;
        }

        .back-link:hover {
            text-decoration: underline;
        }
    </style>
</head>

<body>
    <div class="glass-background"></div>

    <header class="header">
        <a href="../index.html" class="logo">Ayato Studio</a>
        <nav class="nav">
            <a href="../index.html#about">About</a>
            <a href="../index.html#works">Works</a>
            <a href="../index.html#contact">Contact</a>
        </nav>
    </header>

    <main class="article-container">
        <header class="article-header">
            <h1 class="article-title">【解説】Attention Is All You Need</h1>
            <div class="article-meta">
                <span>Category: Machine Learning Paper</span>
            </div>
        </header>

        <article class="article-content">
            <p>現在のテキスト生成AI（GPT系・LLaMA系など）が大量のVRAMを消費する最大要因の一つは、Attention機構が要求するメモリ量が大きいからではないか？と考えています。</p>
            <p>また、大規模言語モデル並みの精度はTransformerモデルの登場前でも理論上可能だったものの、並列処理ができない順次処理であったため計算時間が現実的でなかった。それをVRAMを消費することで解決した、という理解で進めていきます。
            </p>

            <h2>用語解説</h2>

            <h3>Sequence Transduction Model (系列変換モデル)</h3>
            <p><strong>概要:</strong>
                ある順序を持つデータ列（入力系列）を、別の順序を持つデータの列（出力系列）に変換する機械学習モデルのこと。特徴としては入力と出力の長さが異なっていても処理できること。特に自然言語処理（NLP）の分野で中心的な役割を果たす。
            </p>

            <p><strong>主な用途:</strong></p>
            <ul>
                <li>系列変換(Seq2Seq): 機械翻訳、文章要約、音声認識、対話型チャットボットなど</li>
            </ul>

            <h3>代表的なアーキテクチャ</h3>
            <ul>
                <li><strong>RNN/LSTM/GRUベース:</strong> 古くからある手法。データを順番に前から処理するために並列計算が難しく、長文の処理に時間がかかった。</li>
                <li><strong>Transformer(Attention)ベース:</strong>
                    現在の主流。すべての単語を同時に並列処理できるため、高速かつ高精度。そのため、”大規模”言語モデルが可能になった。</li>
            </ul>

            <h2>Attention機構とは？</h2>
            <p>人工ニューラルネットワークにおいて、認知的な注意を模倣するように設計された手法です。入力データのある部分に対する重要度を強化して、他の部分を弱化する効果があります。</p>

            <blockquote>
                従来のRNNは、読んだ本の感想を「一言（固定長ベクトル）」でしか伝えられないシステムでした。 一方、Attention機構は、<strong>「辞書引きシステム」</strong>です。
            </blockquote>

            <h3>Attention機構の2つの主要タイプ</h3>
            <ol>
                <li><strong>Source-Target Attention:</strong> エンコーダ（入力）とデコーダ（出力）の間の注意。入力データと出力データの関係性を学習。</li>
                <li><strong>Self-Attention:</strong> Transformerの中核技術。一つの系列内で、自身の他の部分に注意を向ける。</li>
            </ol>

            <h3>Multi-Head Attention</h3>
            <p>これは、人間が物事を多角的に理解するプロセスを模倣したものです。</p>
            <ul>
                <li>Head 1（演技重視）: 「素晴らしい」→「演技」に80%の注意</li>
                <li>Head 2（技術重視）: 「美しい」→「映像」に85%の注意</li>
                <li>Head 3（感情重視）: 「感動的」→「作品」に75%の注意</li>
            </ul>

            <h2>Transformerモデルのデータフロー</h2>
            <p>論文（特にFigure 1のモデルアーキテクチャ図）に基づき、Transformerモデルにおけるデータの流れを順を追って解説します。</p>

            <h3>1. 入力データの準備</h3>
            <ul>
                <li><strong>エンコーダ側:</strong> 翻訳元の文（例：「Hello world」）</li>
                <li><strong>デコーダ側:</strong> 翻訳生成中の文</li>
            </ul>

            <h3>2. エンコーダスタック (Encoder Stack)</h3>
            <p>Self-Attentionで入力文の中での単語間の関係性を計算し、全結合層を通します。これをN=6回繰り返します。</p>

            <h3>3. デコーダスタック (Decoder Stack)</h3>
            <p>Masked Multi-Head Attentionで「未来の単語」を見ないようにマスク処理し、その後、エンコーダからの出力を使ってCross-Attentionを行います。</p>

            <h2>Encoder-Decoder vs Decoder-only (GPT)</h2>
            <p>「Encoder（入力の理解）」と「Decoder（出力の生成）」という役割分担がTransformerの基本形ですが、なぜGPTのようなモデルは「Decoder」だけで、入力の理解も生成もできてしまうのか。
            </p>

            <h3>1. 「翻訳」と「続きを書く」というタスクの違い</h3>
            <ul>
                <li><strong>翻訳 (Encoder-Decoder):</strong> 「入力」をすべて読んでから「出力」を作る。</li>
                <li><strong>言語モデリング (Decoder-only):</strong> 「ある文章の続き」を作ること。入力と出力は一本の長い系列。</li>
            </ul>

            <h3>2. Masked Self-Attention がすべてを担う</h3>
            <p>Decoder-onlyモデルは、「入力文（プロンプト）」を「生成済みの過去の文章」と見なすことで、Decoderが持つSelf-Attention機能だけで文脈理解（Encoderの役割）を代用できます。
            </p>

            <blockquote>
                Encoder-Decoderモデルが「他人の話（Encoder出力）を聞いて話す」モデルだとすれば、Decoder-onlyモデルは「自分の記憶（過去の系列）を頼りに独り言を話し続ける」モデルだと言えます。
            </blockquote>

        </article>

        <a href="../index.html" class="back-link">← Back to Home</a>
    </main>

    <footer class="footer">
        <p>&copy; 2026 Ayato Studio. Powered by Antigravity Engine.</p>
    </footer>

    <script src="../config.js"></script>
    <script src="../script.js"></script>
</body>

</html>