<!DOCTYPE html>
<html lang="ja">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【深層学習】なぜRNNではダメなのか？LSTMの仕組みを解説 | Ayato AI Studio</title>
    <meta name="description" content="[お題「AI」](https://blog.hatena.ne.jp/-/odai/6802888565255276830)  こんにちは。今回は、時系列データ処理のデファクトスタンダードとして長年君臨してきた **LSTM (Long Short-Term Memory)** について解説します。...">
    <link rel="canonical" href="https://ayato-studio.ai/ja/blog/12262025_深層学習なぜRNNではダメなのかLSTMの仕組みを解説.html">
    <link rel="stylesheet" href="../../index.css">
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&family=Noto+Sans+JP:wght@400;700&display=swap"
        rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', 'Noto Sans JP', sans-serif;
            background: #0a0a0a;
            color: #f0f0f0;
            margin: 0;
            padding: 0;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
        }

        header {
            padding: 2rem 0;
            border-bottom: 1px solid #333;
            margin-bottom: 2rem;
        }

        .back-link {
            color: #00ffaa;
            text-decoration: none;
            font-size: 0.9rem;
        }

        h1 {
            font-size: 2.5rem;
            margin: 0.5rem 0;
            background: linear-gradient(to right, #ffffff, #888888);
            background-clip: text;
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .meta {
            color: #888;
            font-size: 0.9rem;
        }

        .content {
            line-height: 1.8;
            font-size: 1.1rem;
        }

        .content h2 {
            margin-top: 2rem;
            color: #00ffaa;
        }

        .content h3 {
            margin-top: 1.5rem;
            color: #0066ff;
        }

        .content a {
            color: #00ffaa;
        }

        .content code {
            background: #222;
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: monospace;
        }

        .content pre {
            background: #222;
            padding: 1rem;
            border-radius: 8px;
            overflow-x: auto;
        }

        .content img {
            max-width: 100%;
            border-radius: 8px;
            margin: 1rem 0;
        }

        .footer {
            margin-top: 4rem;
            padding-top: 2rem;
            border-top: 1px solid #333;
            text-align: center;
            color: #666;
        }
    </style>
</head>

<body>
    <div class="container">
        <header>
            <a href="../index.html" class="back-link">← Back to Portal</a>
            <h1>【深層学習】なぜRNNではダメなのか？LSTMの仕組みを解説</h1>
            <div class="meta">Published on "12/26/2025 17:45:16" by Ayato</div>
        </header>
        <div class="content">
            <p><a href="https://blog.hatena.ne.jp/-/odai/6802888565255276830">お題「AI」</a></p>
<p>こんにちは。今回は、時系列データ処理のデファクトスタンダードとして長年君臨してきた <strong>LSTM (Long Short-Term Memory)</strong> について解説します。</p>
<p>「RNNと何が違うの？」「数式が多くて挫折した」という方に向けて、<strong>「2つの回路による分業」</strong>という視点から、その内部構造を視覚的に紐解いていきます。</p>
<p><strong>目次</strong></p>
<ul>
<li><a href="#section-1">人間の思考とRNN</a></li>
<li><a href="#section-2">RNNが抱える「長期依存」の問題</a></li>
<li><a href="#section-3">救世主：LSTMの「2つの回路」</a></li>
<li><a href="#section-4">図解：LSTMの「3つのステップ」</a></li>
<li><a href="#section-5">まとめ</a></li>
</ul>
<h2>人間の思考とRNN</h2>
<p>私たちは毎秒、ゼロから思考を開始しているわけではありません。この記事を読むときも、<strong>前の単語の理解に基づいて、今の単語を理解</strong>しています。</p>
<p>従来のニューラルネットワークには、この「持続性」がありませんでした。その欠点を克服するために生まれたのが <strong>リカレントニューラルネットワーク（RNN）</strong> です。</p>
<h3>RNNのループ構造</h3>
<p>RNNは、ループ機能を持つことで情報の永続化を可能にしました。</p>
<p>図の凡例  NN層  演算  RNN x   A  h   =  時間展開  x0   A  h0    x1   A  h1  ...        </p>
<p>図1：RNNは過去の情報を次のステップに引き継ぐ</p>
<h2>RNNが抱える「長期依存」の問題</h2>
<p>RNNは直前の情報は覚えられますが、情報が古くなると忘れてしまいます。「空の色は...」のすぐ後なら「青」と予測できますが、長い文章の最後で冒頭の言葉を思い出すのは苦手です。</p>
<p><strong>RNNの弱点</strong><br />
長期的な依存関係を学習しようとすると、勾配消失などが原因でうまくいかない。パラメータ調整が非常にシビア。</p>
<h2>救世主：LSTMの「2つの回路」</h2>
<p>ここが今回の解説の最重要ポイントです。</p>
<p>RNNの問題を解決するために、LSTMは大胆な構造を採用しました。それは、情報の通り道を<strong>「長期記憶用」と「短期記憶用」の2本に分ける</strong>というアイデアです。</p>
<p>① 長期記憶の回路 (Cell State) ② 短期記憶の回路 (Hidden State / RNN的役割)  普段は使われない「記録保持専用」のライン  メインで活動する「作業用」のライン    忘却ゲート 情報を削除     入力ゲート 情報を記録     出力ゲート 情報を読み出し  </p>
<p>図2：LSTMは「2階建て」の構造になっている</p>
<h3>メイン回路とサブ回路の関係</h3>
<ul>
<li><strong>短期記憶の回路（下段）</strong>: RNNと同じように、日々のタスクや直近の処理を行います。普段はここだけで作業しています。</li>
<li><strong>長期記憶の回路（上段）</strong>: 重要な情報だけをずっと保持しておく「アーカイブ」です。普段は何もしません。</li>
</ul>
<p>短期記憶回路が「あ、これは大事だ！」と思ったときだけ長期回路に書き込み（入力ゲート）、逆に「昔の情報が必要だ！」と思ったときだけ長期回路から情報を引き出す（出力ゲート）のです。</p>
<p>これにより、<strong>「普段は長期記憶に余計な干渉をしない」</strong>ことが保証され、情報の劣化（勾配消失）を防ぐことができるのです。</p>
<h3>核となる概念：セルの状態（Cell State）</h3>
<p>この長期記憶の回路（Cell State）は、しばしば<strong>「ベルトコンベア」</strong>に例えられます。</p>
<p>セルの状態（長期記憶ライン） 情報は変更されずに一直線に流れていく  ×  + 忘却ゲートからの干渉  入力ゲートからの干渉  </p>
<p>図3：ベルトコンベア上を情報は劣化せずに流れる</p>
<h2>図解：LSTMの「3つのステップ」</h2>
<p>では、短期回路（メイン）が長期回路（アーカイブ）に対してどのようにアクセスしているか、3つのゲートの役割を見ていきましょう。</p>
<h3>Step 1: 忘却ゲート（いらない情報を捨てる）</h3>
<p>「前の状態 <img alt="h_{t-1}" src="https://chart.apis.google.com/chart?cht=tx&amp;chl=h_%7Bt-1%7D" />」と「今の入力 <img alt="x_t" src="https://chart.apis.google.com/chart?cht=tx&amp;chl=x_t" />」を見て、<strong>何を忘れるか（0〜1）</strong> を決めます。</p>
<p>ht-1 xt    σ 忘却ゲート層    × 0 = 完全に忘れる 1 = 覚えている </p>
<p>図4：忘却ゲートの処理</p>
<h3>Step 2: 入力ゲート（新しい情報を足す）</h3>
<p>どの情報を更新するかを決め（シグモイド）、新しい候補値を作成（tanh）し、それらをセル状態に追加します。</p>
<p>ht-1 xt        σ 入力ゲート  tanh 候補値     ×      + 新しい情報を追加！ </p>
<p>図5：入力ゲートの処理</p>
<h3>Step 3: 出力ゲート（次の隠れ状態を作る）</h3>
<p>最後に、セル状態をtanhに通して正規化し、出力したい部分だけをシグモイドゲートでフィルタリングして出力します。</p>
<p>h_t = o_t * tanh(C_t)</p>
<h2>まとめ</h2>
<p>LSTMは一見複雑に見えますが、その本質は<strong>「普段使いの回路（RNN）」と「保存用の回路（Cell）」を分けたこと</strong>にあります。</p>
<ul>
<li>普段は短期記憶（RNN回路）で処理を回す。</li>
<li>必要な時だけ、長期記憶（Cell回路）に「書き込み請求」や「読み出し請求」を行う。</li>
</ul>
<p>この分業体制こそが、LSTMが長期依存問題を解決できた最大の理由なのです。</p>
<hr />
<p>Reference:<br />
<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks by Christopher Olah</a></p>
        </div>
        <div class="footer">
            <p>&copy; 2026 Ayato AI Studio. Generated by AI Agent.</p>
        </div>
    </div>
</body>

</html>